{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde76e4d-feb7-4e16-b1bd-8fb6a833cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2. Install packages and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0996bc96-204e-4c32-9963-edb0a821dd63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following additional packages will be installed:\n",
      "  liblept5 libopenjp2-7 libtesseract4 libwebpmux3 tesseract-ocr-eng\n",
      "  tesseract-ocr-osd\n",
      "The following NEW packages will be installed:\n",
      "  liblept5 libopenjp2-7 libtesseract4 libwebpmux3 tesseract-ocr\n",
      "  tesseract-ocr-eng tesseract-ocr-osd\n",
      "0 upgraded, 7 newly installed, 0 to remove and 2 not upgraded.\n",
      "Need to get 7483 kB of archives.\n",
      "After this operation, 23.2 MB of additional disk space will be used.\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libopenjp2-7:amd64.\n",
      "(Reading database ... 141592 files and directories currently installed.)\n",
      "Preparing to unpack .../0-libopenjp2-7_2.4.0-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  3%]\u001b[49m\u001b[39m [##........................................................] \u001b8Unpacking libopenjp2-7:amd64 (2.4.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Selecting previously unselected package libwebpmux3:amd64.\n",
      "Preparing to unpack .../1-libwebpmux3_0.6.1-2.1+deb11u2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 10%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking libwebpmux3:amd64 (0.6.1-2.1+deb11u2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package liblept5:amd64.\n",
      "Preparing to unpack .../2-liblept5_1.79.0-1.1+deb11u1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking liblept5:amd64 (1.79.0-1.1+deb11u1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libtesseract4:amd64.\n",
      "Preparing to unpack .../3-libtesseract4_4.1.1-2.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libtesseract4:amd64 (4.1.1-2.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [################..........................................] \u001b8Selecting previously unselected package tesseract-ocr-eng.\n",
      "Preparing to unpack .../4-tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Selecting previously unselected package tesseract-ocr-osd.\n",
      "Preparing to unpack .../5-tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [########################..................................] \u001b8Selecting previously unselected package tesseract-ocr.\n",
      "Preparing to unpack .../6-tesseract-ocr_4.1.1-2.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 45%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking tesseract-ocr (4.1.1-2.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [############################..............................] \u001b8Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 55%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libopenjp2-7:amd64 (2.4.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libwebpmux3:amd64 (0.6.1-2.1+deb11u2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 72%]\u001b[49m\u001b[39m [##########################################................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up liblept5:amd64 (1.79.0-1.1+deb11u1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [##############################################............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libtesseract4:amd64 (4.1.1-2.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [##################################################........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up tesseract-ocr (4.1.1-2.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 97%]\u001b[49m\u001b[39m [########################################################..] \u001b8Processing triggers for man-db (2.9.4-2) ...\n",
      "Processing triggers for libc-bin (2.31-13+deb11u11) ...\n",
      "ldconfig: /lib/libnvonnxparser.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer_vc_plugin.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer_dispatch.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvparsers.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer_lean.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer_plugin.so.8 is not a symbolic link\n",
      "\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JThe following additional packages will be installed:\n",
      "  libarchive-dev libleptonica-dev\n",
      "The following NEW packages will be installed:\n",
      "  libarchive-dev libleptonica-dev libtesseract-dev\n",
      "0 upgraded, 3 newly installed, 0 to remove and 2 not upgraded.\n",
      "Need to get 3465 kB of archives.\n",
      "After this operation, 15.7 MB of additional disk space will be used.\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libarchive-dev:amd64.\n",
      "(Reading database ... 141702 files and directories currently installed.)\n",
      "Preparing to unpack .../libarchive-dev_3.4.3-2+deb11u2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libarchive-dev:amd64 (3.4.3-2+deb11u2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libleptonica-dev:amd64.\n",
      "Preparing to unpack .../libleptonica-dev_1.79.0-1.1+deb11u1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libleptonica-dev:amd64 (1.79.0-1.1+deb11u1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Selecting previously unselected package libtesseract-dev:amd64.\n",
      "Preparing to unpack .../libtesseract-dev_4.1.1-2.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking libtesseract-dev:amd64 (4.1.1-2.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Setting up libleptonica-dev:amd64 (1.79.0-1.1+deb11u1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libarchive-dev:amd64 (3.4.3-2+deb11u2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [########################################..................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libtesseract-dev:amd64 (4.1.1-2.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Processing triggers for man-db (2.9.4-2) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JSelecting previously unselected package poppler-data.\n",
      "(Reading database ... 141834 files and directories currently installed.)\n",
      "Preparing to unpack .../poppler-data_0.4.10-1_all.deb ...\n",
      "Unpacking poppler-data (0.4.10-1) ...\n",
      "Selecting previously unselected package libnspr4:amd64.\n",
      "Preparing to unpack .../libnspr4_2%3a4.29-1_amd64.deb ...\n",
      "Unpacking libnspr4:amd64 (2:4.29-1) ...\n",
      "Selecting previously unselected package libnss3:amd64.\n",
      "Preparing to unpack .../libnss3_2%3a3.61-1+deb11u4_amd64.deb ...\n",
      "Unpacking libnss3:amd64 (2:3.61-1+deb11u4) ...\n",
      "Selecting previously unselected package libpoppler102:amd64.\n",
      "Preparing to unpack .../libpoppler102_20.09.0-3.1+deb11u1_amd64.deb ...\n",
      "Unpacking libpoppler102:amd64 (20.09.0-3.1+deb11u1) ...\n",
      "Selecting previously unselected package poppler-utils.\n",
      "Preparing to unpack .../poppler-utils_20.09.0-3.1+deb11u1_amd64.deb ...\n",
      "Unpacking poppler-utils (20.09.0-3.1+deb11u1) ...\n",
      "Setting up poppler-data (0.4.10-1) ...\n",
      "Setting up libnspr4:amd64 (2:4.29-1) ...\n",
      "Setting up libnss3:amd64 (2:3.61-1+deb11u4) ...\n",
      "Setting up libpoppler102:amd64 (20.09.0-3.1+deb11u1) ...\n",
      "Setting up poppler-utils (20.09.0-3.1+deb11u1) ...\n",
      "Processing triggers for libc-bin (2.31-13+deb11u11) ...\n",
      "ldconfig: /lib/libnvonnxparser.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer_vc_plugin.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer_dispatch.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvparsers.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer_lean.so.8 is not a symbolic link\n",
      "\n",
      "ldconfig: /lib/libnvinfer_plugin.so.8 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.9.4-2) ...\n",
      "Processing triggers for fontconfig (2.13.1-4.2) ...\n",
      "E: Failed to fetch https://deb.debian.org/debian-security/pool/updates/main/libx/libxml2/libxml2-dev_2.9.10%2bdfsg-6.7%2bdeb11u6_amd64.deb  404  Not Found [IP: 151.101.2.132 443]\n",
      "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-0.3.61-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-google-vertexai\n",
      "  Downloading langchain_google_vertexai-2.0.24-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.42-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.11.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (3.11.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (9.0.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.1.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core) (4.13.0)\n",
      "Collecting bottleneck<2.0.0,>=1.4.2 (from langchain-google-vertexai)\n",
      "  Downloading bottleneck-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting google-cloud-aiplatform<2.0.0,>=1.92.0 (from langchain-google-vertexai)\n",
      "  Downloading google_cloud_aiplatform-1.94.0-py2.py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from langchain-google-vertexai) (2.19.0)\n",
      "Collecting httpx<0.29.0,>=0.28.0 (from langchain-google-vertexai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.6 (from langchain-google-vertexai)\n",
      "  Downloading numexpr-2.10.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: pyarrow<20.0.0,>=19.0.1 in /opt/conda/lib/python3.10/site-packages (from langchain-google-vertexai) (19.0.1)\n",
      "Collecting validators<1,>=0.22.0 (from langchain-google-vertexai)\n",
      "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (2.24.2)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (3.20.3)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (3.31.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (1.14.2)\n",
      "Requirement already satisfied: shapely<3.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (2.0.7)\n",
      "Collecting google-genai<2.0.0,>=1.0.0 (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai)\n",
      "  Downloading google_genai-1.16.1-py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (0.16)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (1.7.1)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx<0.29.0,>=0.28.0->langchain-google-vertexai)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (3.10)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<0.29.0,>=0.28.0->langchain-google-vertexai)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (1.69.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (1.49.0rc1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (2.9.0.post0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (0.14.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (15.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (1.3.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (0.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.92.0->langchain-google-vertexai) (1.17.0)\n",
      "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.61-py3-none-any.whl (438 kB)\n",
      "Downloading langchain_google_vertexai-2.0.24-py3-none-any.whl (100 kB)\n",
      "Downloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading bottleneck-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (357 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading google_cloud_aiplatform-1.94.0-py2.py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.42-py3-none-any.whl (360 kB)\n",
      "Downloading numexpr-2.10.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (397 kB)\n",
      "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Downloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
      "Downloading google_genai-1.16.1-py3-none-any.whl (196 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading orjson-3.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: validators, pypdf, orjson, numexpr, mypy-extensions, marshmallow, httpx-sse, h11, bottleneck, async-timeout, typing-inspect, requests-toolbelt, httpcore, pydantic-settings, httpx, dataclasses-json, langsmith, google-genai, langchain-core, langchain-text-splitters, google-cloud-aiplatform, langchain-google-vertexai, langchain, langchain-community\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "  Attempting uninstall: requests-toolbelt\n",
      "    Found existing installation: requests-toolbelt 0.10.1\n",
      "    Uninstalling requests-toolbelt-0.10.1:\n",
      "      Successfully uninstalled requests-toolbelt-0.10.1\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.86.0\n",
      "    Uninstalling google-cloud-aiplatform-1.86.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.86.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed async-timeout-4.0.3 bottleneck-1.5.0 dataclasses-json-0.6.7 google-cloud-aiplatform-1.94.0 google-genai-1.16.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.0 langchain-0.3.25 langchain-community-0.3.24 langchain-core-0.3.61 langchain-google-vertexai-2.0.24 langchain-text-splitters-0.3.8 langsmith-0.3.42 marshmallow-3.26.1 mypy-extensions-1.1.0 numexpr-2.10.2 orjson-3.10.18 pydantic-settings-2.9.1 pypdf-5.5.0 requests-toolbelt-1.0.0 typing-inspect-0.9.0 validators-0.35.0\n"
     ]
    }
   ],
   "source": [
    "# Install Vertex AI SDK & Other dependencies\n",
    "\n",
    "!sudo apt -y -qq install tesseract-ocr\n",
    "!sudo apt -y -qq install libtesseract-dev\n",
    "!sudo apt-get -y -qq install poppler-utils #required by PyPDF2 for page count and other pdf utilities\n",
    "!sudo apt-get -y -qq install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig\n",
    "!pip install langchain langchain-community langchain-core langchain-google-vertexai pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b36a8e4-ea7b-410b-ae11-1275b662ce1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e22b08-4adc-4fb9-95b5-ec7fb7f6a261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc6f525-9786-4569-aaac-9c005db08b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Task 3. Load the model and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6044c57-29c6-461f-9c3c-08053e150264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_google_vertexai import ChatVertexAI, VertexAI, VertexAIEmbeddings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f784fe50-d70f-44e4-bdb3-f16ac91ffec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"[qwiklabs-gcp-04-72beeead10e0]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"[qwiklabs-gcp-04-72beeead10e0]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
    "\n",
    "import vertexai\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf761e4-ea9a-43bd-a3f5-9d285445777d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Listed 0 items.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai models list --region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d03b7b86-4b03-43e7-80f3-a2465a78da56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The sentence describes a fox jumping over a dog.\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 16, 'candidates_token_count': 11, 'total_token_count': 27, 'prompt_tokens_details': [{'modality': 1, 'token_count': 16}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 11}], 'thoughts_token_count': 0, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.23195778239857068, 'model_name': 'gemini-2.0-flash'}, id='run--b60b0646-69a7-4f0a-9bcb-62feb08de6b6-0', usage_metadata={'input_tokens': 16, 'output_tokens': 11, 'total_tokens': 27})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Load the pre-trained text generation model named \"gemini-1.5-pro\" using \"ChatVertexAI\" class.\n",
    "\n",
    "\n",
    "# Load the Gemini 1.5 Pro model\n",
    "# LLM model\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-2.0-flash\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "my_text = \"Summarize the following sentence: The quick brown fox jumps over the lazy dog.?\"\n",
    "\n",
    "llm.invoke(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597fe978-801d-4e91-bbdd-470b55d19843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Download a PDF file from specified URL and save it in \"data\" directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd0e50b-3ee8-47fc-a275-0ace00df6609",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded and saved to: data/practitioners_guide_to_mlops_whitepaper.pdf\n"
     ]
    }
   ],
   "source": [
    "#TODO: Download a PDF file from specified URL and save it in \"data\" directory.\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Create the \"data\" directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# URL of the PDF\n",
    "url = \"https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf\"\n",
    "\n",
    "# Path to save the file\n",
    "save_path = \"data/practitioners_guide_to_mlops_whitepaper.pdf\"\n",
    "\n",
    "# Download the PDF\n",
    "response = requests.get(url)\n",
    "with open(save_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(f\"PDF downloaded and saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342c9662-2947-4a57-8b2e-21428465bd55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 32.0.1 which is incompatible.\n",
      "kfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Vertex AI SDK, LangChain and dependencies\n",
    "%pip install --upgrade --quiet google-cloud-aiplatform langchain langchain-core langchain-text-splitters langchain-google-vertexai langchain-community faiss-cpu langchain-chroma pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ea6626-63db-4643-a549-b6d3380706cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import (\n",
    "    ConversationChain,\n",
    "    LLMChain,\n",
    "    RetrievalQA,\n",
    "    SimpleSequentialChain,\n",
    ")\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_google_vertexai import ChatVertexAI, VertexAI, VertexAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "505db845-4750-4c7d-9405-efd0b0ea7519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18acd5-29e8-4d16-ac00-0d27b740f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 4. Generate summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bcab2e7-469d-4264-9504-2701d35606dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 37 pages.\n",
      "Sample page content:\n",
      "\n",
      "Practitioners guide to MLOps:\n",
      "A framework for continuous \n",
      "delivery and automation of\n",
      "machine learning.\n",
      "White paper\n",
      "May 2021\n",
      "Authors:\n",
      "Khalid Salama,\n",
      "Jarek Kazmierczak,\n",
      "Donna Schut\n"
     ]
    }
   ],
   "source": [
    "#TODO: Load the PDF file and split it into individual pages.\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Load the PDF file\n",
    "loader = PyMuPDFLoader(\"data/practitioners_guide_to_mlops_whitepaper.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages.\")\n",
    "print(\"Sample page content:\\n\")\n",
    "print(documents[0].page_content[:500])  # Preview first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf503c23-52ac-40df-a39b-2e54fc3d7a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split into 86 chunks.\n",
      "\n",
      "Preview of first chunk:\n",
      "\n",
      "Practitioners guide to MLOps:\n",
      "A framework for continuous \n",
      "delivery and automation of\n",
      "machine learning.\n",
      "White paper\n",
      "May 2021\n",
      "Authors:\n",
      "Khalid Salama,\n",
      "Jarek Kazmierczak,\n",
      "Donna Schut\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\nSplit into {len(split_docs)} chunks.\")\n",
    "print(\"\\nPreview of first chunk:\\n\")\n",
    "print(split_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3826880c-14ae-4d77-b876-15b2963fe70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the document\n",
    "loader = PyMuPDFLoader(\"data/practitioners_guide_to_mlops_whitepaper.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Extract just the first 3 pages\n",
    "first_three_pages = documents[:3]\n",
    "\n",
    "# Split each page into chunks if needed (optional for short pages)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "texts = text_splitter.split_documents(first_three_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de464893-f82f-4be1-a37e-d8e4d0f57d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prompt design with Stuffing Chain\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "  \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24bec1e2-dbec-447f-a4f1-1565784623df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: Set up a summarization chain using the stuff method. Incorporate the model loaded earlier into the summarization chain to enhance the quality of the summary.\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "stuff_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c852a651-97f3-4150-a6aa-7749ef80f677",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following text delimited by triple backquotes.\n",
      "              Return your response in bullet points which covers the key points of the text.\n",
      "              ```Practitioners guide to MLOps:\n",
      "A framework for continuous \n",
      "delivery and automation of\n",
      "machine learning.\n",
      "White paper\n",
      "May 2021\n",
      "Authors:\n",
      "Khalid Salama,\n",
      "Jarek Kazmierczak,\n",
      "Donna Schut\n",
      "\n",
      "Table of Contents\n",
      "Executive summary\t\n",
      "3\n",
      "Overview of MLOps lifecycle and core capabilities\t\n",
      "4\n",
      "Deep dive of MLOps processes\t\n",
      "15\n",
      "Putting it all together\t\n",
      "34\n",
      "Additional resources\t\n",
      "36\n",
      "Building an ML-enabled system\t\n",
      "6\n",
      "The MLOps lifecycle\t\n",
      "7\n",
      "MLOps: An end-to-end workflow\t\n",
      "8\n",
      "MLOps capabilities\t\n",
      "9\n",
      "      Experimentation\t\n",
      "11\n",
      "      Data processing\t\n",
      "11\n",
      "      Model training\t\n",
      "11\n",
      "      Model evaluation\t\n",
      "12\n",
      "      Model serving\t\n",
      "12\n",
      "      Online experimentation\t\n",
      "13\n",
      "      Model monitoring\t\n",
      "13\n",
      "      ML pipelines\t\n",
      "13\n",
      "      Model registry\t\n",
      "14\n",
      "      Dataset and feature repository\t\n",
      "14\n",
      "      ML metadata and artifact tracking\t\n",
      "15\n",
      "ML development\t\n",
      "16\n",
      "Training operationalization\t\n",
      "18\n",
      "Continuous training\t\n",
      "20\n",
      "Model deployment\t\n",
      "23\n",
      "Prediction serving\t\n",
      "25\n",
      "Continuous monitoring\t\n",
      "26\n",
      "Data and model management\t\n",
      "29\n",
      "      Dataset and feature management\t\n",
      "29\n",
      "             Feature management\t\n",
      "30\n",
      "             Dataset management\t\n",
      "31\n",
      "      Model management\t\n",
      "32\n",
      "             ML metadata tracking\t\n",
      "32\n",
      "             Model governance\t\n",
      "33\n",
      "\n",
      "Executive summary\n",
      "Across industries, DevOps and DataOps have been widely adopted as methodologies to improve quality and re­\n",
      "duce the time to market of software engineering and data engineering initiatives. With the rapid growth in machine \n",
      "learning (ML) systems, similar approaches need to be developed in the context of ML engineering, which handle the \n",
      "unique complexities of the practical applications of ML. This is the domain of MLOps. MLOps is a set of standard­\n",
      "ized processes and technology capabilities for building, deploying, and operationalizing ML systems rapidly and \n",
      "reliably.]\n",
      "We previously published Google Cloud’s AI Adoption Framework to provide guidance for technology leaders who \n",
      "want to build an effective artificial intelligence (AI) capability in order to transform their business. That framework \n",
      "covers AI challenges around people, data, technology, and process, structured in six different themes: learn, lead, \n",
      "access, secure, scale, and automate. \n",
      "The current document takes a deeper dive into the themes of scale and automate to illustrate the requirements for \n",
      "building and operationalizing ML systems. Scale concerns the extent to which you use cloud managed ML services \n",
      "that scale with large amounts of data and large numbers of data processing and ML jobs, with reduced operational \n",
      "overhead. Automate concerns the extent to which you are able to deploy, execute, and operate technology for data\n",
      "\n",
      "processing and ML pipelines in production efficiently, frequently, and reliably.\n",
      "We outline an MLOps framework that defines core processes and technical capabilities. Organizations can use this \n",
      "framework to help establish mature MLOps practices for building and operationalizing ML systems. Adopting the \n",
      "framework can help organizations improve collaboration between teams, improve the reliability and scalability of ML \n",
      "systems, and shorten development cycle times. These benefits in turn drive innovation and help gain overall busi­\n",
      "ness value from investments in ML.\n",
      "This document is intended for technology leaders and enterprise architects who want to understand MLOps. It’s also \n",
      "for teams who want details about what MLOps looks like in practice. The document assumes that readers are famil­\n",
      "iar with basic machine learning concepts and with development and deployment practices such as CI/CD.\n",
      "The document is in two parts. The first part, an overview of the MLOps lifecycle, is for all readers. It introduces \n",
      "MLOps processes and capabilities and why they’re important for successful adoption of ML-based systems.\n",
      "The second part is a deep dive on the MLOps processes and capabilities. This part is for readers who want to un­\n",
      "derstand the concrete details of tasks like running a continuous training pipeline, deploying a model, and monitoring \n",
      "predictive performance of an ML model.\n",
      "3```\n",
      "              BULLET POINT SUMMARY:\n",
      "  \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "--- Bullet Point Summary of First 3 Pages ---\n",
      "\n",
      "*   **MLOps Definition:** MLOps is a set of standardized processes and technologies for rapidly and reliably building, deploying, and operationalizing ML systems.\n",
      "*   **Purpose:** This white paper provides a framework for continuous delivery and automation of machine learning, focusing on scaling and automating ML systems.\n",
      "*   **Target Audience:** Technology leaders, enterprise architects, and ML teams who want to understand and implement MLOps. Assumes familiarity with ML concepts and CI/CD.\n",
      "*   **AI Adoption Framework:** Builds upon Google Cloud's AI Adoption Framework, specifically addressing the \"scale\" and \"automate\" themes.\n",
      "*   **Benefits of MLOps:** Improved collaboration, reliability, scalability, and shorter development cycles, leading to business value from ML investments.\n",
      "*   **MLOps Lifecycle Overview:** The first part of the document introduces MLOps processes and capabilities for successful ML adoption.\n",
      "*   **MLOps Process Deep Dive:** The second part details specific tasks like continuous training, model deployment, and performance monitoring.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = stuff_chain.run(texts)\n",
    "print(\"\\n--- Bullet Point Summary of First 3 Pages ---\\n\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883eb22-65cc-4af9-95d6-4873d6ddb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 5. Add tools to the LLM for multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eaed7db5-a814-4059-a946-d5602231dbb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: Load the PDF file and split it into individual pages.\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2ba26f8-5f27-4e21-aec1-227147178e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: Add the tools to the LLM created earlier and invoke it with the following query. Print the result in the console.\n",
    "from langchain_core.messages import HumanMessage, ToolMessage, AIMessage\n",
    "query = \"What is 3 * 12?\"\n",
    "messages = [HumanMessage(query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea98878d-d529-4e71-8867-7f1addd41b54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 3.0, 'b': 12.0},\n",
       "  'id': '5e96a483-83a6-4bcd-992e-184aa9a90b77',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Iterate through the tools in the response, invoke the tools and append the response to the messages object.\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "always_multiply_llm = llm.bind_tools([multiply], tool_choice=\"multiply\")\n",
    "always_call_tool_llm = llm.bind_tools([add, multiply], tool_choice=\"any\")\n",
    "query = \"What is 3 * 12?\"\n",
    "\n",
    "llm_with_tools.invoke(query).tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6071ad9f-f764-4eac-a5b2-4c7bcd583269",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 3 * 12?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"a\": 3.0, \"b\": 12.0}'}}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 68, 'candidates_token_count': 5, 'total_token_count': 73, 'prompt_tokens_details': [{'modality': 1, 'token_count': 68}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 5}], 'thoughts_token_count': 0, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -3.176135942339897e-06, 'model_name': 'gemini-2.0-flash'}, id='run--c1db2769-393f-48cd-a9e4-d0d9eafc237a-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3.0, 'b': 12.0}, 'id': '66c93742-114e-4ea3-b8f6-734595724d97', 'type': 'tool_call'}], usage_metadata={'input_tokens': 68, 'output_tokens': 5, 'total_tokens': 73}),\n",
       " ToolMessage(content='36', tool_call_id='66c93742-114e-4ea3-b8f6-734595724d97')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Invoke the LLM with the solution of the tool and the original message and print the final user response.\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb7cd5-a892-4ddc-8e81-92da15d645e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
