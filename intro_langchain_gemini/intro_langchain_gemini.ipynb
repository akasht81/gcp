{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1uzXWPtI1b_"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "359697d5"
   },
   "source": [
    "# Getting Started with LangChain ü¶úÔ∏èüîó + Gemini API in Vertex AI\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Forchestration%2Fintro_langchain_gemini.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/orchestration/intro_langchain_gemini.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b24d7ab7a7a5"
   },
   "source": [
    "| Authors |\n",
    "| --- |\n",
    "| [Rajesh Thallam](https://github.com/RajeshThallam) |\n",
    "| [Holt Skinner](https://github.com/holtskinner) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11d788b0"
   },
   "source": [
    "### **What is LangChain?**\n",
    "\n",
    "> LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "**TL;DR** LangChain makes the complicated parts of working & building with language models easier. It helps do this in two ways:\n",
    "\n",
    "1. **Integration** - Bring external data, such as your files, other applications, and API data, to LLMs\n",
    "2. **Agents** - Allows LLMs to interact with its environment via decision making and use LLMs to help decide which action to take next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpilrb5XVT_k"
   },
   "source": [
    "To build effective Generative AI applications, it is key to enable LLMs to interact with external systems. This makes models data-aware and agentic, meaning they can understand, reason, and use data to take action in a meaningful way. The external systems could be public data corpus, private knowledge repositories, databases, applications, APIs, or access to the public internet via Google Search.\n",
    "\n",
    "Here are a few patterns where LLMs can be augmented with other systems:\n",
    "\n",
    "- Convert natural language to SQL, executing the SQL on database, analyze and present the results\n",
    "- Calling an external webhook or API based on the user query\n",
    "- Synthesize outputs from multiple models, or chain the models in a specific order\n",
    "\n",
    "It may look trivial to plumb these calls together and orchestrate them but it becomes a mundane task to write glue code again and again e.g. for every different data connector or a new model. That's where LangChain comes in!\n",
    "\n",
    "![Augmenting LLMs](https://storage.googleapis.com/gweb-cloudblog-publish/images/Patterns_augmenting_LLMs_with_external_syste.max-900x900.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aATSgLjXVVY0"
   },
   "source": [
    "### **Why LangChain?**\n",
    "\n",
    "LangChain's modular implementation of components and common patterns combining these components makes it easier to build complex applications based on LLMs. LangChain enables these models to connect to data sources and systems as agents to take action.\n",
    "\n",
    "1. **Components** are abstractions that works to bring external data, such as your documents, databases, applications,APIs to language models. LangChain makes it easy to swap out abstractions and components necessary to work with LLMs.\n",
    "\n",
    "2. **Agents** enable language models to communicate with its environment, where the model then decides the next action to take. LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
    "\n",
    "Though LLMs can be straightforward (text-in, text-out) you'll quickly run into friction points that LangChain helps with once you develop more complicated applications.\n",
    "\n",
    "### LangChain & Vertex AI\n",
    "\n",
    "[Vertex AI Generative AI models](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview) ‚Äî Gemini and Embeddings ‚Äî are officially integrated with the [LangChain Python SDK](https://python.langchain.com/en/latest/index.html), making it convenient to build applications using Gemini models with the ease of use and flexibility of LangChain.\n",
    "\n",
    "- [LangChain Google Integrations](https://python.langchain.com/v0.2/docs/integrations/platforms/google/)\n",
    "\n",
    "---\n",
    "\n",
    "_Note: This notebook does not cover all aspects of LangChain. Its contents have been curated to get you to building & impact as quick as possible. For more, please check out [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50b19d12"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "This notebook provides an introductory understanding of [LangChain](https://langchain.com/) components and use cases of LangChain with the Gemini API in Vertex AI.\n",
    "\n",
    "- Introduce LangChain components\n",
    "- Showcase LangChain + Gemini API in Vertex AI - Text, Chat and Embedding\n",
    "- Summarizing a large text\n",
    "- Question/Answering from PDF (retrieval based)\n",
    "- Chain LLMs with Google Search\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "\n",
    "- Adapted from [LangChain Cookbook](https://github.com/gkamradt/langchain-tutorials) from [Greg Kamradt](https://twitter.com/GregKamradt)\n",
    "- [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)\n",
    "- [LangChain Python Documentation](https://python.langchain.com/en/latest/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e985f332"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ohPUPez8imvE",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.13.0 requires kubernetes<31,>=8.0.0, but you have kubernetes 32.0.1 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 2.2.6 which is incompatible.\n",
      "apache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.7 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "dataproc-jupyter-plugin 0.1.80 requires aiohttp~=3.9.5, but you have aiohttp 3.10.5 which is incompatible.\n",
      "dataproc-jupyter-plugin 0.1.80 requires pydantic~=1.10.0, but you have pydantic 2.11.5 which is incompatible.\n",
      "ibis-framework 7.1.0 requires numpy<2,>=1, but you have numpy 2.2.6 which is incompatible.\n",
      "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 19.0.1 which is incompatible.\n",
      "matplotlib 3.7.3 requires numpy<2,>=1.20, but you have numpy 2.2.6 which is incompatible.\n",
      "numba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "scipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.25.7 which is incompatible.\n",
      "tensorboardx 2.6 requires protobuf<4,>=3.8.0, but you have protobuf 4.25.7 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.7 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.7 which is incompatible.\n",
      "tensorflow-transform 0.14.0 requires numpy<2,>=1.16, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.7 which is incompatible.\n",
      "ydata-profiling 4.6.0 requires numpy<1.26,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\n",
      "ydata-profiling 4.6.0 requires pydantic<2,>=1.8.1, but you have pydantic 2.11.5 which is incompatible.\n",
      "ydata-profiling 4.6.0 requires seaborn<0.13,>=0.10.1, but you have seaborn 0.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Vertex AI SDK, LangChain and dependencies\n",
    "%pip install --upgrade --quiet google-cloud-aiplatform langchain langchain-core langchain-text-splitters langchain-google-vertexai langchain-community faiss-cpu langchain-chroma pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "034fe628"
   },
   "source": [
    "### Authenticating your notebook environment\n",
    "\n",
    "- If you are using **Colab** to run this notebook, run the cell below and continue.\n",
    "- If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6d83f50a3d3f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f33c406cb7c"
   },
   "source": [
    "- If you are running this notebook in a local development environment:\n",
    "  - Install the [Google Cloud SDK](https://cloud.google.com/sdk).\n",
    "  - Obtain authentication credentials. Create local credentials by running the following command and following the oauth2 flow (read more about the command [here](https://cloud.google.com/sdk/gcloud/reference/beta/auth/application-default/login)):\n",
    "\n",
    "    ```bash\n",
    "    gcloud auth application-default login\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a92ac8ea"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7aedf28aa5e"
   },
   "source": [
    "**Colab only:** Run the following cell to initialize the Vertex AI SDK. For Vertex AI Workbench, you don't need to run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "4f6ab09a187c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"[qwiklabs-gcp-02-7ec35d313232]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"[qwiklabs-gcp-02-7ec35d313232]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
    "\n",
    "import vertexai\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Smo-7TpE4B22",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import (\n",
    "    ConversationChain,\n",
    "    LLMChain,\n",
    "    RetrievalQA,\n",
    "    SimpleSequentialChain,\n",
    ")\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_google_vertexai import ChatVertexAI, VertexAI, VertexAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dd3a375d3b6"
   },
   "source": [
    "Define LangChain Models using the Gemini API in Vertex AI for Text, Chat and Vertex AI Embeddings for Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "eVpPcvsrkzCk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM model\n",
    "llm = VertexAI(\n",
    "    model_name=\"gemini-2.0-flash\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Chat\n",
    "chat = ChatVertexAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "# Embedding\n",
    "embeddings = VertexAIEmbeddings(\"text-embedding-005\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05bb564d"
   },
   "source": [
    "## LangChain Components\n",
    "\n",
    "Let's take a quick tour of LangChain framework and concepts to be aware of. LangChain offers a variety of modules that can be used to create language model applications. These modules can be combined to create more complex applications, or can be used individually for simpler applications.\n",
    "\n",
    "![LangChain Components](https://storage.googleapis.com/gweb-cloudblog-publish/images/Figure-3-LangChain_Concepts.max-1300x1300.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyGZZEWlZOe0"
   },
   "source": [
    "- **Models** are the building block of LangChain providing an interface to different types of AI models. Large Language Models (LLMs), Chat and Text Embeddings models are supported model types.\n",
    "- **Prompts** refers to the input to the model, which is typically constructed from multiple components. LangChain provides interfaces to construct and work with prompts easily - Prompt Templates, Example Selectors and Output Parsers.\n",
    "- **Memory** provides a construct for storing and retrieving messages during a conversation which can be either short term or long term.\n",
    "- **Indexes** help LLMs interact with documents by providing a way to structure them. LangChain provides Document Loaders to load documents, Text Splitters to split documents into smaller chunks, Vector Stores to store documents as embeddings, and Retrievers to fetch relevant documents.\n",
    "- **Chains** let you combine modular components (or other chains) in a specific order to complete a task.\n",
    "- **Agents** are a powerful construct in LangChain allowing LLMs to communicate with external systems via Tools and observe and decide on the best course of action to complete a given task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZfIhJSMjDLV"
   },
   "source": [
    "## Schema - Nuts and Bolts of working with LLMs\n",
    "\n",
    "### Text\n",
    "\n",
    "Text is the natural language way to interact with LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "8e0dc06c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saturday\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You'll be working with simple strings (that'll soon grow in complexity!)\n",
    "my_text = \"What day comes after Friday?\"\n",
    "\n",
    "llm.invoke(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f39eb39"
   },
   "source": [
    "### Chat Messages\n",
    "\n",
    "Chat is like text, but specified with a message type (System, Human, AI)\n",
    "\n",
    "- **System** - Helpful context that tells the AI what to do\n",
    "- **Human** - Messages intended to represent the user\n",
    "- **AI** - Messages showing what the AI responded with\n",
    "\n",
    "For more information, see [LangChain Documentation for Chat Models](https://python.langchain.com/docs/modules/model_io/chat).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "qhQOijKAt1ta",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 1, 'candidates_token_count': 10, 'total_token_count': 11, 'prompt_tokens_details': [{'modality': 1, 'token_count': 1}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 10}], 'thoughts_token_count': 0, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.11962493658065795, 'model_name': 'gemini-2.0-flash'}, id='run--fc109da3-d2ee-4ae5-ad27-6123272ce6eb-0', usage_metadata={'input_tokens': 1, 'output_tokens': 10, 'total_tokens': 11})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke([HumanMessage(content=\"Hello\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "878d6a36",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why not try a Caprese salad with fresh tomatoes, mozzarella, and basil?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = chat.invoke(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"\n",
    "        ),\n",
    "        HumanMessage(content=\"I like tomatoes, what should I eat?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a425aaa"
   },
   "source": [
    "You can also pass more chat history w/ responses from the AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "3bNQxPln7wC0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ingredients for a classic tomato sandwich are pretty simple:\n",
      "\n",
      "*   **Ripe Tomatoes:** The star of the show! Choose juicy, flavorful tomatoes. Heirloom varieties are often a great choice.\n",
      "*   **Bread:** White bread is traditional, but you can use any bread you like‚Äîsourdough, whole wheat, or even toasted.\n",
      "*   **Mayonnaise:** A generous spread of mayonnaise is essential for many.\n",
      "*   **Salt and Pepper:** To season the tomatoes.\n",
      "*   **Optional:** Lettuce, red onion or bacon for extra flavour.\n"
     ]
    }
   ],
   "source": [
    "res = chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"What are the ingredients required for making a tomato sandwich?\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66bf9634"
   },
   "source": [
    "### Documents\n",
    "\n",
    "Document in LangChain refers to an unstructured text consisting of `page_content` referring to the content of the data and `metadata` (data describing attributes of page content).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "150e8759",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019}, page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(\n",
    "    page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "    metadata={\n",
    "        \"my_document_id\": 234234,\n",
    "        \"my_document_source\": \"The LangChain Papers\",\n",
    "        \"my_document_create_time\": 1680013019,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2b70f23"
   },
   "source": [
    "### Text Embedding Model\n",
    "\n",
    "[Embeddings](https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings) are a way of representing data‚Äìalmost any kind of data, like text, images, videos, users, music, whatever‚Äìas points in space where the locations of those points in space are semantically meaningful. Embeddings transform your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Vectors are often used when comparing two pieces of text together. An [embedding](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture) is a relatively low-dimensional space into which you can translate high-dimensional vectors.\n",
    "\n",
    "[LangChain Text Embedding Model](https://python.langchain.com/v0.2/docs/how_to/embed_text) is integrated with [Vertex AI Embedding API for Text](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings).\n",
    "\n",
    "_BTW: Semantic means 'relating to meaning in language or logic.'_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "a2c85e7e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Hi! It's time for the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "ddc5a368",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embedding is length 768\n",
      "Here's a sample: [-0.02593194507062435, -0.019493747502565384, -0.03560653328895569, -0.02734011597931385, -0.017964690923690796]...\n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print(f\"Your embedding is length {len(text_embedding)}\")\n",
    "print(f\"Here's a sample: {text_embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c38fe99f"
   },
   "source": [
    "## Prompts\n",
    "\n",
    "Prompts are text used as instructions to your model. For more details have a look at the notebook [Intro to Prompt Design](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "PrvHxWMidmTU",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The problem is that if today is Monday, tomorrow would logically be Tuesday, not Wednesday.\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74988254"
   },
   "source": [
    "### **Prompt Template**\n",
    "\n",
    "[Prompt Template](https://python.langchain.com/v0.1/docs/modules/model_io/#prompt-templates) is an object that helps to create prompts based on a combination of user input, other non-static information and a fixed template string.\n",
    "\n",
    "Think of it as an [`f-string`](https://realpython.com/python-f-strings/) in Python but for prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "abcc212d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "I really want to travel to Rome. What should I do there?\n",
      "\n",
      "Respond in one short sentence\n",
      "\n",
      "-----------\n",
      "LLM Output: Explore ancient ruins, admire Renaissance art, indulge in delicious food, and soak up the vibrant Roman atmosphere.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location=\"Rome\")\n",
    "\n",
    "output = llm.invoke(final_prompt)\n",
    "\n",
    "print(f\"Final Prompt: {final_prompt}\")\n",
    "print(\"-----------\")\n",
    "print(f\"LLM Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed40bac2"
   },
   "source": [
    "### **Example Selectors**\n",
    "\n",
    "[Example selectors](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/) are an easy way to select from a series of examples to dynamically place in-context information into your prompt. Often used when the task is nuanced or has a large list of examples.\n",
    "\n",
    "Check out different types of example selectors [here](https://python.langchain.com/docs/how_to/example_selectors/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "aaf36cd9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "12b4798b",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "input not a numpy array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_3394/2383875040.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m example_selector = SemanticSimilarityExampleSelector.from_examples(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# This is the list of examples available to select from.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# This is the embedding class used to produce embeddings which are used to measure semantic similarity.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/example_selectors/semantic_similarity.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, examples, embeddings, vectorstore_cls, k, input_keys, example_keys, vectorstore_kwargs, **vectorstore_cls_kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mExampleSelector\u001b[0m \u001b[0minstantiated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbacked\u001b[0m \u001b[0mby\u001b[0m \u001b[0ma\u001b[0m \u001b[0mvector\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \"\"\"\n\u001b[1;32m    170\u001b[0m         \u001b[0mstring_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_example_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         vectorstore = vectorstore_cls.from_texts(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mstring_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvectorstore_cls_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         )\n\u001b[1;32m    174\u001b[0m         return cls(\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \"\"\"\n\u001b[1;32m   1043\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m         return cls.__from(\n\u001b[0m\u001b[1;32m   1045\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m             \u001b[0mnormalize_L2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize_L2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0mdistance_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistance_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         )\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0mvecstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvecstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, texts, embeddings, metadatas, ids)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# Add to the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_L2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_L2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Add information to docstore and index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/faiss/class_wrappers.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/faiss/swigfaiss_avx2.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m  12723\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 12724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: input not a numpy array"
     ]
    }
   ],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    embeddings,\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # This is the number of examples to produce.\n",
    "    k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "2cf30107",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_selector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m similar_prompt \u001b[38;5;241m=\u001b[39m FewShotPromptTemplate(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# The object that will help select examples\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     example_selector\u001b[38;5;241m=\u001b[39m\u001b[43mexample_selector\u001b[49m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Your prompt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     example_prompt\u001b[38;5;241m=\u001b[39mexample_prompt,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Customizations that will be added to the top and bottom of your prompt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGive the location an item is usually found in\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;132;01m{noun}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# What inputs your prompt will receive\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnoun\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'example_selector' is not defined"
     ]
    }
   ],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "369442bb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select a noun!\n",
    "my_noun = \"student\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bb910f2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.invoke(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8474c91d"
   },
   "source": [
    "### **Output Parsers**\n",
    "\n",
    "[Output Parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/) help to format the output of a model. Usually used for structured output.\n",
    "\n",
    "Two main ideas:\n",
    "\n",
    "**1. Format Instructions**: An autogenerated prompt that tells the LLM how to format it's response based off desired result\n",
    "\n",
    "**2. Parser**: A method to extract model's text output into a desired structure (usually json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "fa59be3f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How you would like your response structured. This is basically a fancy prompt template\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"bad_string\", description=\"This a poorly formatted user input string\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"good_string\", description=\"This is your response, a reformatted response\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "d1079f0a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# See the prompt template you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "9aaae5be",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will be given a poorly formatted string from a user.\n",
      "Reformat it and make sure all the words are spelled correctly including country, city and state names\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "welcom to dbln!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly including country, city and state names\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "prompt_value = prompt.format(user_input=\"welcom to dbln!\")\n",
    "\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "b116bb23",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"welcom to dbln!\",\\n\\t\"good_string\": \"Welcome to Dublin!\"\\n}\\n```'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output = llm.invoke(prompt_value)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "985aa814",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'welcom to dbln!', 'good_string': 'Welcome to Dublin!'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b43cec2"
   },
   "source": [
    "## Indexes\n",
    "\n",
    "[Indexes](https://docs.langchain.com/docs/components/indexing/) refer to ways to structure documents for LLMs to work with them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3f904e9"
   },
   "source": [
    "### **Document Loaders**\n",
    "\n",
    "Document loaders are ways to import data from other sources. See the [growing list](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) of document loaders here. There are more on [LlamaIndex](https://llamahub.ai/) as well that work with LangChain Document Loaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ee693520",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"http://www.paulgraham.com/worked.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "88d89ad7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "e814f930",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 comments\n",
      "Here's a sample:\n",
      "\n",
      "What I Worked On\n",
      "\n",
      "February 2021Before college the two main things I worked on, outside of school,\n",
      "were writing and programming. I didn't write essays.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(data)} comments\")\n",
    "print(f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e9601db"
   },
   "source": [
    "### **Text Splitters**\n",
    "\n",
    "[Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) are a way to deal with input token limits of LLMs by splitting text into chunks.\n",
    "\n",
    "There are many ways you could split your text into chunks, experiment with [different ones](https://python.langchain.com/docs/modules/data_connection/document_transformers/) to see which is best for your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CbA6yXonidz9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"http://www.paulgraham.com/worked.html\")\n",
    "pg_work = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "d19acb18",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(pg_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "e3090f05",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 79 documents\n"
     ]
    }
   ],
   "source": [
    "print(f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "87a0f45a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      "What I Worked On \n",
      "\n",
      "February 2021Before college the two main things I worked on, outside of school,\n",
      "were writing and programming. I didn't write essays. I wrote what\n",
      "beginning writers were supposed to write then, and probably still\n",
      "are: short stories. My stories were awful. They had hardly any plot,\n",
      "just characters with strong feelings, which I imagined made them\n",
      "deep.The first programs I tried writing were on the IBM 1401 that our\n",
      "school district used for what was then called \"data processing.\"\n",
      "This was in 9th grade, so I was 13 or 14. The school district's\n",
      "1401 happened to be in the basement of our junior high school, and\n",
      "my friend Rich Draves and I got permission to use it. It was like\n",
      "a mini Bond villain's lair down there, with all these alien-looking\n",
      "machines ‚Äî CPU, disk drives, printer, card reader ‚Äî sitting up\n",
      "on a raised floor under bright fluorescent lights.The language we used was an early version of Fortran. You had to\n",
      "type programs on punch cards, then stack them in the card reader\n"
     ]
    }
   ],
   "source": [
    "print(\"Preview:\")\n",
    "print(texts[0].page_content, \"\\n\")\n",
    "print(texts[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f85defb"
   },
   "source": [
    "### **Retrievers**\n",
    "\n",
    "[Retrievers](https://python.langchain.com/docs/modules/data_connection/retrievers) are a way of storing data such that it can be queried by a language model. Easy way to combine documents with language models.\n",
    "\n",
    "There are [many different types of retrievers](https://python.langchain.com/docs/modules/data_connection/retrievers.html#advanced-retrieval-types), the most widely supported is the `VectorStoreRetriever`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Vu_uBr4rKHvP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"http://www.paulgraham.com/worked.html\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnifR4tZkV5P"
   },
   "source": [
    "Here we use [Facebook AI Similarity Search (FAISS)](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/), a library and a vector database for similarity search and clustering of dense vectors. To generate dense vectors, a.k.a. embeddings, we use [LangChain text embeddings model with Vertex AI Embeddings for Text](https://python.langchain.com/docs/integrations/text_embedding/google_vertex_ai_palm) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "1dab1c20",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "input not a numpy array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_3394/2380901203.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Split your docs into texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Embed your texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/vectorstores/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0;31m# should be used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \"\"\"\n\u001b[1;32m   1043\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m         return cls.__from(\n\u001b[0m\u001b[1;32m   1045\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m             \u001b[0mnormalize_L2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize_L2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0mdistance_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistance_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         )\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0mvecstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvecstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, texts, embeddings, metadatas, ids)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# Add to the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_L2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_L2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Add information to docstore and index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/faiss/class_wrappers.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/faiss/swigfaiss_avx2.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m  12723\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 12724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: input not a numpy array"
     ]
    }
   ],
   "source": [
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Embed your texts\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e62372be"
   },
   "outputs": [],
   "source": [
    "# Init your retriever. Asking for just 1 document back\n",
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3846a3b5"
   },
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\n",
    "    \"what types of things did the author want to develop or build?\"\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24193139"
   },
   "source": [
    "### Vector Stores\n",
    "\n",
    "[Vector Store](https://python.langchain.com/docs/modules/data_connection/vectorstores) is a common type of index or a database to store vectors (numerical embeddings). Conceptually, think of them as tables with a column for embeddings (vectors) and a column for metadata.\n",
    "\n",
    "Example\n",
    "\n",
    "| Embedding                                             | Metadata           |\n",
    "| ----------------------------------------------------- | ------------------ |\n",
    "| `[-0.00015641732898075134, -0.003165106289088726, ...]` | `{'date' : '1/2/23}` |\n",
    "| `[-0.00035465431654651654, 1.4654131651654516546, ...]` | `{'date' : '1/3/23}` |\n",
    "\n",
    "- [Chroma](https://www.trychroma.com/) & [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) are easy to work with locally.\n",
    "- [Vertex AI Vector Search (Matching Engine)](https://cloud.google.com/blog/products/ai-machine-learning/vertex-matching-engine-blazing-fast-and-massively-scalable-nearest-neighbor-search) is fully managed vector store on Google Cloud, developers can just add the embeddings to its index and issue a search query with a key embedding for the blazingly fast vector search.\n",
    "\n",
    "<br/>\n",
    "\n",
    "LangChain VectorStore is [integrated with Vertex AI Vector Search](https://python.langchain.com/v0.2/docs/integrations/vectorstores/google_vertex_ai_vector_search/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "3c5533ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"http://www.paulgraham.com/worked.html\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "661fdf19",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 52 documents\n"
     ]
    }
   ],
   "source": [
    "print(f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "e99ac0ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_list = embeddings.embed_documents([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "89e7758c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 52 embeddings\n",
      "Here's a sample of one: [-0.005950769409537315, 0.007125218398869038, 0.005224744323641062]...\n"
     ]
    }
   ],
   "source": [
    "print(f\"You have {len(embedding_list)} embeddings\")\n",
    "print(f\"Here's a sample of one: {embedding_list[0][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ac358c5"
   },
   "source": [
    "VectorStore stores your embeddings (‚òùÔ∏è) and makes them easily searchable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9b9b79b"
   },
   "source": [
    "## Memory\n",
    "\n",
    "[Memory](https://python.langchain.com/docs/modules/memory/) is the concept of storing and retrieving data in the process of a conversation. Memory helps LLMs remember information you've chatted about in the past or more complicated information retrieval.\n",
    "\n",
    "There are many types of memory, explore [the documentation](https://python.langchain.com/docs/modules/memory/) to see which one fits your use case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f43b49da"
   },
   "source": [
    "### ConversationBufferMemory\n",
    "\n",
    "Memory keeps conversation state throughout a user's interactions with a language model. `ConversationBufferMemory` memory allows for storing of messages and then extracts the messages in a variable.\n",
    "\n",
    "We'll use `ConversationChain` to have a conversation and load context from memory. We will look into Chains in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "893a18c1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_3394/1992978922.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  llm=llm, verbose=True, memory=ConversationBufferMemory()\n",
      "/var/tmp/ipykernel_3394/1992978922.py:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! It's a pleasure to be chatting with you. I'm ready to answer any questions you might have, or just generally engage in conversation. I'm running on a large language model, which means I've been trained on a massive dataset of text and code, allowing me to understand and generate human-like text in response to a wide range of prompts. I can do things like summarize factual topics, create stories, translate languages, and even write different kinds of creative content. Let me know what's on your mind!\\n\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=True, memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "iG0y5VXL8R6Y",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! It's a pleasure to be chatting with you. I'm ready to answer any questions you might have, or just generally engage in conversation. I'm running on a large language model, which means I've been trained on a massive dataset of text and code, allowing me to understand and generate human-like text in response to a wide range of prompts. I can do things like summarize factual topics, create stories, translate languages, and even write different kinds of creative content. Let me know what's on your mind!\n",
      "\n",
      "Human: What is the capital of France?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Ah, a classic! The capital of France is **Paris**.\\n\\nIt's a fascinating city, rich in history and culture. Did you know that Paris wasn't always called Paris? It originated as a Celtic settlement known as Lutetia, which was conquered by the Romans in the 1st century AD. The city gradually evolved and was eventually renamed Paris after the Parisii, a Gallic tribe that inhabited the region.\\n\\nParis is divided into 20 arrondissements, or districts, arranged in a spiral pattern starting from the city center. And, of course, it's home to iconic landmarks like the Eiffel Tower, the Louvre Museum (home to the Mona Lisa!), Notre Dame Cathedral, and the Arc de Triomphe.\\n\\nIs there anything else you'd like to know about Paris or France in general? I'd be happy to elaborate!\\n\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "R7TfAEQDoPK_",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! It's a pleasure to be chatting with you. I'm ready to answer any questions you might have, or just generally engage in conversation. I'm running on a large language model, which means I've been trained on a massive dataset of text and code, allowing me to understand and generate human-like text in response to a wide range of prompts. I can do things like summarize factual topics, create stories, translate languages, and even write different kinds of creative content. Let me know what's on your mind!\n",
      "\n",
      "Human: What is the capital of France?\n",
      "AI: Ah, a classic! The capital of France is **Paris**.\n",
      "\n",
      "It's a fascinating city, rich in history and culture. Did you know that Paris wasn't always called Paris? It originated as a Celtic settlement known as Lutetia, which was conquered by the Romans in the 1st century AD. The city gradually evolved and was eventually renamed Paris after the Parisii, a Gallic tribe that inhabited the region.\n",
      "\n",
      "Paris is divided into 20 arrondissements, or districts, arranged in a spiral pattern starting from the city center. And, of course, it's home to iconic landmarks like the Eiffel Tower, the Louvre Museum (home to the Mona Lisa!), Notre Dame Cathedral, and the Arc de Triomphe.\n",
      "\n",
      "Is there anything else you'd like to know about Paris or France in general? I'd be happy to elaborate!\n",
      "\n",
      "Human: What are some popular places I can see in France?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'France is brimming with incredible places to visit! Here are a few popular and diverse options, spanning different regions and interests:\\n\\n*   **Paris (√éle-de-France):** Obviously! Beyond the iconic landmarks I mentioned before, consider exploring Montmartre (the artists\\' district), taking a boat tour on the Seine River, visiting the Palace of Versailles (a short train ride away), and indulging in the city\\'s amazing culinary scene. Don\\'t forget the charming cafes and boulangeries!\\n\\n*   **The French Riviera (Provence-Alpes-C√¥te d\\'Azur):** Think sunshine, beaches, and glamour. Nice, Cannes, and Monaco are the main draws. You can explore the picturesque villages perched on hillsides, relax on the beaches, visit art museums (like the Matisse Museum in Nice), and enjoy the vibrant nightlife. The Cannes Film Festival in May brings an extra buzz, but expect higher prices.\\n\\n*   **The Loire Valley (Centre-Val de Loire):** Known as the \"Garden of France,\" this region is famous for its stunning ch√¢teaux (castles). Many of these were built during the Renaissance period and are incredibly well-preserved. Ch√¢teau de Chambord is the largest and arguably most impressive, but Chenonceau (built across the Cher River) and Villandry (with its beautiful gardens) are also highlights. It\\'s a great area for wine tasting too.\\n\\n*   **Provence (Provence-Alpes-C√¥te d\\'Azur):** Lavender fields stretching as far as the eye can see (best seen in late June and July), charming villages like Gordes and Roussillon, Roman ruins (like the Pont du Gard aqueduct), and the vibrant city of Avignon (with its Palais des Papes) are all part of Provence\\'s allure. Don\\'t forget to try the regional cuisine, with its emphasis on fresh herbs and olive oil.\\n\\n*   **Normandy (Normandy):** Steeped in history, particularly WWII history, Normandy is home to the D-Day landing beaches, such as Omaha Beach and Utah Beach. You can visit museums, memorials, and cemeteries to learn about this pivotal moment in history. Beyond that, Normandy also boasts beautiful coastline, charming towns like Honfleur, and the iconic Mont Saint-Michel, a tidal island topped with a medieval abbey.\\n\\n*   **The Alps (Auvergne-Rh√¥ne-Alpes):** If you\\'re into outdoor activities, the French Alps offer fantastic skiing and snowboarding in the winter and hiking, mountain biking, and paragliding in the summer. Chamonix, at the foot of Mont Blanc (the highest peak in Western Europe), is a popular base.\\n\\n*   **Bordeaux (Nouvelle-Aquitaine):** A must-visit for wine lovers! The city of Bordeaux itself is beautiful, with grand architecture and a vibrant cultural scene. But the surrounding vineyards are the real draw, offering tours and tastings of some of the world\\'s finest wines.\\n\\nTo give you more tailored recommendations, what are you most interested in seeing and doing? History? Art? Nature? Wine? Relaxation? Knowing your preferences will help me suggest the perfect places for you!\\n'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What are some popular places I can see in France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "HoJ9HnR6oKbT",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! It's a pleasure to be chatting with you. I'm ready to answer any questions you might have, or just generally engage in conversation. I'm running on a large language model, which means I've been trained on a massive dataset of text and code, allowing me to understand and generate human-like text in response to a wide range of prompts. I can do things like summarize factual topics, create stories, translate languages, and even write different kinds of creative content. Let me know what's on your mind!\n",
      "\n",
      "Human: What is the capital of France?\n",
      "AI: Ah, a classic! The capital of France is **Paris**.\n",
      "\n",
      "It's a fascinating city, rich in history and culture. Did you know that Paris wasn't always called Paris? It originated as a Celtic settlement known as Lutetia, which was conquered by the Romans in the 1st century AD. The city gradually evolved and was eventually renamed Paris after the Parisii, a Gallic tribe that inhabited the region.\n",
      "\n",
      "Paris is divided into 20 arrondissements, or districts, arranged in a spiral pattern starting from the city center. And, of course, it's home to iconic landmarks like the Eiffel Tower, the Louvre Museum (home to the Mona Lisa!), Notre Dame Cathedral, and the Arc de Triomphe.\n",
      "\n",
      "Is there anything else you'd like to know about Paris or France in general? I'd be happy to elaborate!\n",
      "\n",
      "Human: What are some popular places I can see in France?\n",
      "AI: France is brimming with incredible places to visit! Here are a few popular and diverse options, spanning different regions and interests:\n",
      "\n",
      "*   **Paris (√éle-de-France):** Obviously! Beyond the iconic landmarks I mentioned before, consider exploring Montmartre (the artists' district), taking a boat tour on the Seine River, visiting the Palace of Versailles (a short train ride away), and indulging in the city's amazing culinary scene. Don't forget the charming cafes and boulangeries!\n",
      "\n",
      "*   **The French Riviera (Provence-Alpes-C√¥te d'Azur):** Think sunshine, beaches, and glamour. Nice, Cannes, and Monaco are the main draws. You can explore the picturesque villages perched on hillsides, relax on the beaches, visit art museums (like the Matisse Museum in Nice), and enjoy the vibrant nightlife. The Cannes Film Festival in May brings an extra buzz, but expect higher prices.\n",
      "\n",
      "*   **The Loire Valley (Centre-Val de Loire):** Known as the \"Garden of France,\" this region is famous for its stunning ch√¢teaux (castles). Many of these were built during the Renaissance period and are incredibly well-preserved. Ch√¢teau de Chambord is the largest and arguably most impressive, but Chenonceau (built across the Cher River) and Villandry (with its beautiful gardens) are also highlights. It's a great area for wine tasting too.\n",
      "\n",
      "*   **Provence (Provence-Alpes-C√¥te d'Azur):** Lavender fields stretching as far as the eye can see (best seen in late June and July), charming villages like Gordes and Roussillon, Roman ruins (like the Pont du Gard aqueduct), and the vibrant city of Avignon (with its Palais des Papes) are all part of Provence's allure. Don't forget to try the regional cuisine, with its emphasis on fresh herbs and olive oil.\n",
      "\n",
      "*   **Normandy (Normandy):** Steeped in history, particularly WWII history, Normandy is home to the D-Day landing beaches, such as Omaha Beach and Utah Beach. You can visit museums, memorials, and cemeteries to learn about this pivotal moment in history. Beyond that, Normandy also boasts beautiful coastline, charming towns like Honfleur, and the iconic Mont Saint-Michel, a tidal island topped with a medieval abbey.\n",
      "\n",
      "*   **The Alps (Auvergne-Rh√¥ne-Alpes):** If you're into outdoor activities, the French Alps offer fantastic skiing and snowboarding in the winter and hiking, mountain biking, and paragliding in the summer. Chamonix, at the foot of Mont Blanc (the highest peak in Western Europe), is a popular base.\n",
      "\n",
      "*   **Bordeaux (Nouvelle-Aquitaine):** A must-visit for wine lovers! The city of Bordeaux itself is beautiful, with grand architecture and a vibrant cultural scene. But the surrounding vineyards are the real draw, offering tours and tastings of some of the world's finest wines.\n",
      "\n",
      "To give you more tailored recommendations, what are you most interested in seeing and doing? History? Art? Nature? Wine? Relaxation? Knowing your preferences will help me suggest the perfect places for you!\n",
      "\n",
      "Human: What question did I ask first?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You first asked, \"Hi there!\"\\n'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What question did I ask first?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f29fc79c"
   },
   "source": [
    "## Chains ‚õìÔ∏è‚õìÔ∏è‚õìÔ∏è\n",
    "\n",
    "Chains are a generic concept in LangChain allowing to combine different LLM calls and action automatically.\n",
    "\n",
    "Ex:\n",
    "\n",
    "```\n",
    "Summary #1, Summary #2, Summary #3 --> Final Summary\n",
    "```\n",
    "\n",
    "There are [many applications of chains](https://python.langchain.com/docs/modules/chains) search to see which are best for your use case.\n",
    "\n",
    "We'll cover a few of them:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c34ba415"
   },
   "source": [
    "### 1. Simple Sequential Chains\n",
    "\n",
    "[Sequential chains](https://python.langchain.com/en/latest/modules/chains/generic/sequential_chains.html) are a series of chains, called in deterministic order. `SimpleSequentialChain` are easy chains where each step uses the output of an LLM as an input into another. Good for breaking up tasks (and keeping the LLM focused).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "43d4494a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_3394/1937169603.py:10: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  location_chain = LLMChain(llm=llm, prompt=prompt_template)\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "b6c8e00f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "7e0b83f2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "7d19c64d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mOkay, for Rome, a classic dish has to be **Pasta alla Carbonara**. \n",
      "\n",
      "It's simple, decadent, and truly Roman. Forget cream! Authentic Carbonara consists of pasta (usually spaghetti or rigatoni), guanciale (cured pork cheek), eggs, Pecorino Romano cheese, and black pepper. It's a perfect representation of Roman culinary tradition.\n",
      "\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mOkay, here's a quick and easy recipe for **Pasta alla Carbonara:**\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "*   4 oz Guanciale (or Pancetta as a substitute), cut into cubes\n",
      "*   8 oz Spaghetti\n",
      "*   2 Large Eggs + 1 Large Egg Yolk\n",
      "*   1 cup Pecorino Romano Cheese, grated, plus extra for serving\n",
      "*   Black Pepper, freshly ground\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1.  **Cook the Guanciale:** In a pan over medium heat, cook the guanciale until crispy, rendering its fat. Remove the guanciale and set aside, leaving the rendered fat in the pan.\n",
      "2.  **Cook the Pasta:** While the guanciale is cooking, bring a large pot of salted water to a boil. Add the spaghetti and cook according to package directions until al dente.\n",
      "3.  **Make the Sauce:** In a bowl, whisk together the eggs, egg yolk, Pecorino Romano cheese, and a generous amount of black pepper.\n",
      "4.  **Combine and Serve:** Drain the pasta, reserving about 1/2 cup of pasta water. Add the pasta to the pan with the guanciale fat. Toss to coat. Remove the pan from the heat.\n",
      "5.  Quickly pour the egg and cheese mixture over the hot pasta and toss vigorously to create a creamy sauce. Add a little pasta water at a time if needed to loosen the sauce. Be careful not to cook the eggs too much ‚Äì you want a creamy, not scrambled, sauce.\n",
      "6.  Serve immediately, topped with the crispy guanciale and more Pecorino Romano cheese and black pepper. Buon appetito!\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "review = overall_chain.invoke(\"Rome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6191bf5"
   },
   "source": [
    "### 2. Summarization Chain\n",
    "\n",
    "[Summarization Chain](https://python.langchain.com/docs/modules/chains/popular/summarize) easily runs through a long numerous documents and get a summary.\n",
    "\n",
    "There are multiple chain types such as Stuffing, Map-Reduce, Refine, Map-Rerank. Check out [documentation](https://python.langchain.com/docs/modules/chains/how_to/) for other chain types besides `map-reduce`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "6f218c3e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words in the document = 13654\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"How to use Grounding for your LLMs with text embeddings | Google Cloud BlogJump to ContentCloudBlogContact sales Get started for free CloudBlogSolutions & technologyAI & Machine LearningAPI ManagementApplication DevelopmentApplication ModernizationChrome EnterpriseComputeContainers & KubernetesData AnalyticsDatabasesDevOps & SREMaps & GeospatialSecuritySecurity & IdentityThreat IntelligenceInfrastructureInfrastructure ModernizationNetworkingProductivity & CollaborationSAP on Google CloudStorage & Data TransferSustainabilityEcosystemIT LeadersIndustriesFinancial ServicesHealthcare & Life SciencesManufacturingMedia & EntertainmentPublic SectorRetailSupply ChainTelecommunicationsPartnersStartups & SMBTraining & CertificationsInside Google CloudGoogle Cloud Next & EventsGoogle Cloud ConsultingGoogle Maps PlatformGoogle WorkspaceDevelopers & PractitionersTransform with Google CloudContact sales Get started for free AI & Machine LearningVertex AI Embeddings for Text: Grounding LLMs made easyMay 25, 2023Kaz SatoDeveloper Advocate, Cloud AIIvan CheungDeveloper Programs Engineer, Google CloudMany people are now starting to think about how to bring Gen AI and large language models (LLMs) to production services. You may be wondering \"How to integrate LLMs or AI chatbots with existing IT systems, databases and business data?\", \"We have thousands of products. How can I let LLM memorize them all precisely?\", or \"How to handle the hallucination issues in AI chatbots to build a reliable\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"issues in AI chatbots to build a reliable service?\". Here is a quick solution: grounding with embeddings and vector search.What is grounding? What are embedding and vector search? In this post, we will learn these crucial concepts to build reliable Gen AI services for enterprise use. But before we dive deeper, here is an example:Semantic search on 8 million Stack Overflow questions in milliseconds.¬†(Try the demo here)This demo is available as a public live demo here. Select \"STACKOVERFLOW\" and enter any coding question as a query, so it runs a text search on 8 million questions posted on Stack Overflow.The following points make this demo unique:LLM-enabled semantic search: The 8 million Stack Overflow questions and query text are both interpreted by Vertex AI Generative AI models. The model understands the meaning and intent (semantics) of the text and code snippets in the question body at librarian-level precision. The demo leverages this ability for finding highly relevant questions and goes far beyond simple keyword search in terms of user experience. For example, if you enter \"How do I write a class that instantiates only once\", then the demo shows \"How to create a singleton class\" at the top, as the model knows their meanings are the same in the context of computer programming.Grounded to business facts: In this demo, we didn't try having the LLM to memorize the 8 million items with complex and lengthy prompt engineering. Instead, we attached the Stack Overflow dataset\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Instead, we attached the Stack Overflow dataset to the model as an external memory using vector search, and used no prompt engineering. This means, the outputs are all directly \"grounded\" (connected) to the business facts, not the artificial output from the LLM. So the demo is ready to be served today as a production service with mission critical business responsibility. It does not suffer from the limitation of LLM memory or unexpected behaviors of LLMs such as the hallucinations.Scalable and fast: The demo gives you the search results in tens of milliseconds while retaining the deep semantic understanding capability. Also, the demo is capable of scaling out to handle thousands of search queries every second. This is enabled with the combination of LLM embeddings and Google AI's vector search technology.The key enablers of this solution are 1) the embeddings generated with Vertex AI Embeddings for Text and 2) fast and scalable vector search by Vertex AI Vector Search. Let's start by taking a look at these technologies.First key enabler: Vertex AI Embeddings for TextOn May 10, 2023, Google Cloud announced the following Embedding APIs for Text and Image. They are available on Vertex AI Model Garden.¬†Embeddings for Text : The API takes text input up to 3,072 input tokens and outputs 768 dimensional text embeddings, and is available as a public preview. As of May 10, 2023, the pricing is $0.0001 per 1000 characters (the latest pricing is available on the Pricing for Generative\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"is available on the Pricing for Generative AI models page).Embeddings for Image: Based on Google AI's Contrastive Captioners (CoCa) model, the API takes either image or text input and outputs 1024 dimensional image/text multimodal embeddings, available to trusted testers. This API outputs so-called \"multimodal\" embeddings, enabling multimodal queries where you can execute semantic search on images by text queries, or vise-versa. We will feature this API in another blog post soon.In this blog, we will explain more about why embeddings are useful and show you how to build and an application leveraging Embeddings API for Text. In a future blog post, we will provide a deep dive on Embeddings API for Image.Embeddings API for Text on Vertex AI Model GardenWhat is embeddings?So, what are semantic search and embeddings? With the rise of LLMs, why is it becoming important for IT engineers and ITDMs to understand how they work? To learn it, please take a look at this video from a Google I/O 2023 session for 5 minutes:Also, Foundational courses: Embeddings on Google Machine Learning DYM crash course and Meet AI‚Äôs multitool: Vector embeddings by Dale Markowitz are great materials to learn more about embeddings.LLM text embedding business use casesWith the embedding API, you can apply the innovation of embeddings, combined with the LLM capability, to various text processing tasks, such as:LLM-enabled Semantic Search: text embeddings can be used to represent both the meaning and intent of\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"used to represent both the meaning and intent of a user's query and documents in the embedding space. Documents that have similar meaning to the user's query intent will be found fast with vector search technology. The model is capable of generating text embeddings that capture the subtle nuances of each sentence and paragraphs in the document.LLM-enabled Text Classification: LLM text embeddings can be used for text classification with a deep understanding of different contexts without any training or fine-tuning (so-called zero-shot learning). This wasn't possible with the past language models without task-specific training.LLM-enabled Recommendation: The text embedding can be used for recommendation systems as a strong feature for training recommendation models such as Two-Tower model. The model learns the relationship between the query and candidate embeddings, resulting in next-gen user experience with semantic product recommendation.LLM-enabled Clustering, Anomaly Detection, Sentiment Analysis, and more, can be also handled with the LLM-level deep semantics understanding.Sorting 8 million texts at \"librarian-level\" precision¬†Vertex AI Embeddings for Text has an embedding space with 768 dimensions. As explained in the video above, the space represents a huge map of a wide variety of texts in the world, organized by their meanings. With each input text, the model can find a location (embedding) in the map.The API can take 3,072 input tokens, so it can digest the overall\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3,072 input tokens, so it can digest the overall meaning of a long text and even programming code, and represent it as single embedding. It is like having a librarian knowledgeable about a wide variety of industries, reading through millions of texts carefully, and sorting them with millions of nano-categories that can classify even slight differences of subtle nuances.By visualizing the embedding space, you can actually observe how the model sorts the texts at the \"librarian-level\" precision. Nomic AI provides a platform called Atlas for storing, visualizing and interacting with embedding spaces with high scalability and in a smooth UI, and they worked with Google for visualizing the embedding space of the 8 million Stack Overflow questions. You can try exploring around the space, zooming in and out to each data point on your browser on this page, courtesy of Nomic AI.8 million Stack Overflow questions embedding spaceVisualized by Nomic AI Atlas (Try exploring it here)Examples of the \"librarian-level\" semantic understanding¬†by Embeddings API with Stack Overflow questionsNote that this demo didn't require any training or fine-tuning with computer programming specific datasets. This is the innovative part of the zero-shot learning capability of the LLM; it can be applied to a wide variety of industries, including finance, healthcare, retail, manufacturing, construction, media, and more, for deep semantic search on the industry-focused business documents without spending time\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"business documents without spending time and cost for collecting industry specific datasets and training models.The second key enabler: fast and scalable Vector SearchThe second key enabler of the Stack Overflow demo shown earlier is the vector search technology. This is another innovation we are having in the data science field.The problem is \"how to find similar embeddings in the embedding space\". Since embeddings are vectors, this can be done by calculating the distance or similarity between vectors, as shown below.But this isn't easy when you have millions or billions of embeddings. For example, if you have 8 million embeddings with 768 dimensions, you would need to repeat the calculation in the order of 8 million x 768. This would take a very long time to finish. Actually, when we tried this on BigQuery with one million embeddings five years ago, it took 20 seconds.So the researchers have been studying a technique called Approximate Nearest Neighbor (ANN) for faster search. ANN uses \"vector quantization\" for separating the space into multiple spaces with a tree structure. This is similar to the index in relational databases for improving the query performance, enabling very fast and scalable search with billions of embeddings.With the rise of LLMs, the ANN is getting popular quite rapidly, known as the Vector Search technology.In 2020, Google Research published a new ANN algorithm called ScaNN. It is considered one of the best ANN algorithms in the industry, also the\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"the best ANN algorithms in the industry, also the most important foundation for search and recommendation in major Google services such as Google Search, YouTube and many others.Google Cloud developers can take the full advantage of Google's vector search technology with Vertex AI Vector Search. With this fully managed service, developers can just add the embeddings to its index and issue a search query with a key embedding for the blazingly fast vector search. In the case of the Stack Overflow demo,¬†Vector Search¬†can find relevant questions from 8 million embeddings in tens of milliseconds.With¬†Vector Search, you don't need to spend much time and money building your own vector search service from scratch or using open source tools if your goal is high scalability, availability and maintainability for production systems.Grounding LLM outputs with Vector SearchBy combining the Embeddings API and¬†Vector Search, you can use the embeddings to \"ground\" LLM outputs to real business data with low latency:In the case of the Stack Overflow demo shown earlier, we've built a system with the following architecture.Stack Overflow semantic search demo architectureThe demo architecture has two parts: 1) building a Vector Search index with Vertex AI Workbench and the Stack Overflow dataset on BigQuery (on the right) and 2) processing vector search requests with Cloud Run (on the left) and Vector Search. For the details, please see the sample Notebook on GitHub.Grounding LLMs with LangChain\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Notebook on GitHub.Grounding LLMs with LangChain and Vertex AIIn addition to the architecture used for the Stack Overflow demo, another popular way for grounding is to enter the vector search result into the LLM and let the LLM generate the final answer text for the user. LangChain is a popular tool for implementing this pipeline, and Vertex AI Gen AI embedding APIs and Vector Search are definitely best suited for LangChain integration. In a future blog post, we will explore this topic further. So stay tuned!How to get startedIn this post, we have seen how the combination of Embeddings for Text API¬† and Vector Search allows enterprises to use Gen AI and LLMs in a grounded and reliable way. The fine-grained semantic understanding capability of the API can bring the intelligence to information search and recommendation in a wide variety of businesses, setting a new standard of user experience in enterprise IT systems.To get started, please check out the following resources:Stack Overflow semantic search demo: sample Notebook on GitHubVertex AI Embeddings for Text API documentationVector Search documentationAI & Machine LearningGoogle Cloud advances generative AI at I/O: new foundation models, embeddings, and tuning tools in Vertex AIBy June Yang ‚Ä¢ 5-minute readPosted inAI & Machine LearningDevelopers & PractitionersRelated articlesAI & Machine LearningMistral AI's Le Chat Enterprise and Mistral OCR 25.05 model are available on Google CloudBy Nenshad Bardoliwalla ‚Ä¢ 3-minute\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"on Google CloudBy Nenshad Bardoliwalla ‚Ä¢ 3-minute readAI & Machine LearningAnnouncing Anthropic‚Äôs Claude Opus 4 and Claude Sonnet 4 on Vertex AIBy Nenshad Bardoliwalla ‚Ä¢ 5-minute readAI & Machine LearningTrain AI for less: Improve ML Goodput with elastic training and optimized checkpointingBy Deepak Patil ‚Ä¢ 5-minute readData AnalyticsIntroducing AI.GENERATE_TABLE: creating structured data from gen AI models in BigQueryBy Xi Cheng ‚Ä¢ 4-minute readFooter LinksFollow usGoogle CloudGoogle Cloud ProductsPrivacyTermsCookies management controlsHelpLanguage‚Ä™English‚Ä¨‚Ä™Deutsch‚Ä¨‚Ä™Fran√ßais‚Ä¨‚Ä™ÌïúÍµ≠Ïñ¥‚Ä¨‚Ä™Êó•Êú¨Ë™û‚Ä¨\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_3394/3920030697.py:16: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(texts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The Google Cloud blog post discusses using text embeddings and grounding to integrate LLMs with existing systems, databases, and business data. This approach addresses concerns about LLM accuracy, memorization of large datasets, and hallucination, making LLMs more reliable for production use.\n",
      "\n",
      "\n",
      "Building reliable AI chatbots for enterprise use faces challenges. A key solution is \"grounding,\" achieved through embeddings and vector search. This involves using LLMs to understand the meaning (semantics) of text and code, then using vector search to retrieve relevant information from a knowledge base (like a dataset of Stack Overflow questions). This grounding approach avoids relying solely on the LLM's memory and allows for more accurate and contextually relevant responses.\n",
      "\n",
      "\n",
      "This demo bypasses traditional LLM limitations (memory, hallucinations) by using Google Cloud's Vertex AI Embeddings for Text and Vertex AI Vector Search to directly ground LLM outputs in the Stack Overflow dataset. This allows for fast (tens of milliseconds), scalable (thousands of queries per second), and reliable search results based on semantic understanding, making it production-ready.\n",
      "\n",
      "\n",
      "Google offers an Embeddings API for both text and images. The text API is detailed in this blog post, while the image API (powered by the CoCa model and outputting 1024-dimensional embeddings) will be covered in a future post. Embeddings are crucial for semantic search and various text processing tasks, especially when combined with LLMs, and enable applications like LLM-enabled semantic search. Further learning resources on embeddings are provided.\n",
      "\n",
      "\n",
      "LLM-enabled text embeddings, generated by models like Vertex AI Embeddings for Text, capture nuanced meaning and intent in text, enabling powerful applications like zero-shot text classification, improved recommendation systems, and other semantic-based analyses. These embeddings allow for fast vector search and high-precision sorting of large text datasets by representing text meaning in a high-dimensional space.\n",
      "\n",
      "\n",
      "The Embeddings API can process long texts (up to 3,072 tokens) and create a single embedding that captures nuanced meaning, even in specialized domains like code, without requiring training data. Nomic AI's Atlas platform, used to visualize 8 million Stack Overflow question embeddings, allows users to explore and interact with these semantic spaces, showcasing the API's \"librarian-level\" understanding and applicability across diverse industries for deep semantic search.\n",
      "\n",
      "\n",
      "The Stack Overflow demo leverages Vector Search technology, a key enabler for quickly finding similar embeddings (vectors) in large datasets. Traditional distance calculations are slow at scale. Approximate Nearest Neighbor (ANN) algorithms, like Google's ScaNN, use techniques like vector quantization to create tree structures for faster, scalable search, enabling efficient retrieval of similar data in millions or billions of embeddings.\n",
      "\n",
      "\n",
      "Google's Vertex AI Vector Search leverages their advanced ANN (approximate nearest neighbor) algorithms, powering search and recommendation across Google services. This fully managed service allows developers to quickly and efficiently search through embeddings, like finding relevant Stack Overflow questions from millions of data points in milliseconds. By combining Vector Search with the Embeddings API, users can \"ground\" LLM outputs with real business data, as demonstrated in a Stack Overflow demo architecture utilizing BigQuery, Vertex AI Workbench, Cloud Run, and LangChain. This avoids the need to build a custom vector search solution for high scalability and maintainability.\n",
      "\n",
      "\n",
      "This post introduces using Google Cloud's Embeddings for Text API and Vertex AI Vector Search for grounding LLMs, making Gen AI more reliable. LangChain is mentioned as a tool for implementing this, promising a future blog post detailing its integration with Vertex AI. The post highlights the capabilities of semantic understanding for improving search and recommendation and provides links to resources for getting started, including a Stack Overflow demo notebook.\n",
      "\n",
      "\n",
      "Recent Google Cloud announcements include the availability of Anthropic's Claude Opus 4 and Claude Sonnet 4 on Vertex AI, optimized elastic training and checkpointing for improved ML goodput, and the introduction of AI.GENERATE_TABLE in BigQuery for creating structured data from generative AI models.\n",
      "\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To improve the reliability and accuracy of LLMs in production, Google Cloud advocates \"grounding\" LLM responses using its Embeddings for Text API and Vertex AI Vector Search. This approach uses text embeddings to understand semantic meaning and then uses vector search to retrieve relevant information from a knowledge base (like the Stack Overflow dataset), bypassing LLM memory limitations and hallucinations. This is demonstrated in a Stack Overflow demo and enables applications like LLM-enhanced semantic search. Google\\'s Vertex AI Vector Search, leveraging advanced ANN algorithms like ScaNN, ensures fast and scalable retrieval of similar data, and can be combined with LangChain to enable integration with custom data sources. Recent Google Cloud announcements also include the availability of new models and enhancements to ML infrastructure.\\n'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\n",
    "    \"https://cloud.google.com/blog/products/ai-machine-learning/how-to-use-grounding-for-your-llms-with-text-embeddings\"\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"# of words in the document = {len(documents[0].page_content)}\")\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ta-Vt4t3wTQ7"
   },
   "source": [
    "### 3. Question/Answering Chain\n",
    "\n",
    "[Question Answering Chains](https://python.langchain.com/v0.1/docs/use_cases/question_answering/) easily do QA over a set of documents using QA chain. There are multiple ways to do this with LangChain. We use [**RetrievalQA** chain](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db) which uses `load_qa_chain` under the hood.\n",
    "\n",
    "![QA Process](https://miro.medium.com/v2/resize:fit:2000/format:webp/0*x2f4Es8-NO6zUmks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "3QjgeWSMw0Bb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load GOOG's 10K annual report (92 pages).\n",
    "url = \"https://abc.xyz/assets/investor/static/pdf/20230203_alphabet_10K.pdf\"\n",
    "loader = PyPDFLoader(url)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "OJecKul0xgWT",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents = 263\n"
     ]
    }
   ],
   "source": [
    "# split the documents into chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"# of documents = {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "8V8AoFiQyMSx",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VertexAIEmbeddings(client=<vertexai.language_models.TextEmbeddingModel object at 0x7ff10030a980>, async_client=None, project='qwiklabs-gcp-02-7ec35d313232', location='us-central1', request_parallelism=5, max_retries=6, stop=None, model_name='text-embedding-005', full_model_name=None, client_options=ClientOptions: {'api_endpoint': 'us-central1-aiplatform.googleapis.com', 'client_cert_source': None, 'client_encrypted_cert_source': None, 'quota_project_id': None, 'credentials_file': None, 'scopes': None, 'api_key': None, 'api_audience': None, 'universe_domain': None}, api_endpoint=None, api_transport=None, default_metadata=(), additional_headers=None, client_cert_source=None, credentials=None, endpoint_version='v1beta1', client_preview=None, temperature=None, frequency_penalty=None, presence_penalty=None, max_output_tokens=None, top_p=None, top_k=None, n=1, seed=None, streaming=False, model_family=None, safety_settings=None, tuned_model_name=None, thinking_budget=None, audio_timestamp=None, instance={'max_batch_size': 250, 'batch_size': 250, 'min_batch_size': 5, 'min_good_batch_size': 18, 'lock': <unlocked _thread.lock object at 0x7ff113a5fa80>, 'batch_size_validated': False, 'task_executor': <concurrent.futures.thread.ThreadPoolExecutor object at 0x7ff0efd330a0>, 'get_embeddings_with_retry': <function _TextEmbeddingModel.get_embeddings at 0x7ff1136cb880>})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select embedding engine - we use Vertex AI Embeddings API\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store docs in local VectorStore as index\n",
    "# it may take a while since API is rate limited\n",
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29At6L691XZr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expose index to the retriever\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "ibQdjXjw1foF",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create chain to answer questions\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Uses LLM to synthesize results from the search index.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m qa \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[0;32m----> 5\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm, chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever\u001b[38;5;241m=\u001b[39m\u001b[43mretriever\u001b[49m, return_source_documents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "# Create chain to answer questions\n",
    "\n",
    "# Uses LLM to synthesize results from the search index.\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvjxHX6a105H"
   },
   "outputs": [],
   "source": [
    "query = \"What was Alphabet's net income in 2022?\"\n",
    "result = qa.invoke({\"query\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLQPrNUq2aiF"
   },
   "source": [
    "![executive-overview](https://storage.googleapis.com/github-repo/generative-ai/gemini/orchestration/intro_langchain_gemini/executive-overview.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Of9XWLw9C653"
   },
   "outputs": [],
   "source": [
    "query = \"How much office space reduction took place in 2023?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hb9RHMw5DKjQ"
   },
   "source": [
    "![office-space-reduction](https://storage.googleapis.com/github-repo/generative-ai/gemini/orchestration/intro_langchain_gemini/office-space-reduction.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_langchain_gemini.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
